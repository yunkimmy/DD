\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2024} with \usepackage[nohyperref]{icml2024} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2024}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2024}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Dataset Condensation: Why Learn What You Already Know}

\begin{document}

\twocolumn[
\icmltitle{Dataset Condensation: Why Learn What You Already Know}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2024
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Yun Kim}{yyy}
\icmlauthor{Banseok Kim}{yyy}

\end{icmlauthorlist}

\icmlaffiliation{yyy}{Yonsei University}


\icmlcorrespondingauthor{Yun Kim}{yunkimmy@yonsei.ac.kr}
\icmlcorrespondingauthor{Banseok Kim}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{} % otherwise use the standard text.

\begin{abstract}
Dataset distillation has emerged as a prominent area of research, aiming to compress large, original datasets into compact, lightweight synthetic datasets. Currently, three main methodologies address this problem; however, they all face a critical limitation. While synthetic images perform well under low image-per-class (IPC) settings, their performance quickly plateaus at higher IPC settings and, in some cases, even falls behind that of coreset selection methods. Our analysis reveals that this phenomenon is primarily due to a lack of diversity in the generated images. This redundancy degrades performance, particularly in high IPC settings. To address this fundamental issue, we propose YONSEI, a novel distillation framework that can be seamlessly integrated into existing approaches. YONSEI facilitates the generation of synthetic images that retain high performance across a wide range of IPC settings, effectively mitigating saturation. When incorporated with existing methods, YONSEI achieves state-of-the-art (SOTA) performance in high IPC settings, even surpassing the performance of networks trained on the full dataset. Moreover, our method shows a very strong capability to generalize to unseen architectures, paving the way to make dataset distillation significantly more practical.
\end{abstract}


\section{Introduction}
\label{sec}

Dataset distillation is the task of synthesizing a small dataset while retaining the informational richness of the original dataset. The original paper frames this as a meta-learning problem, where synthetic images are updated to minimize the loss on real data when a network is trained using the synthetic dataset. However, this approach is computationally impractical, as it requires an intractable amount of computation to back-propagate through the network's optimization graph. To address this challenge, surrogate objectives have been proposed. These surrogates embed information from the real data and update the synthetic images to align with the embedded representations. The embedding process can leverage features, gradients, or training trajectories. These methods have demonstrated significant efficiency, achieving strong performance even without directly optimizing the meta-learning objective.

Despite these advances, dataset distillation fails to scale effectively to high image-per-class (IPC) settings. As shown in the graph above, performance plateaus after approximately 50 IPC, indicating that current methodologies do not scale to higher IPC levels. Further investigation reveals that the generated images contain redundant information, leading to repetitive representations. Ironically, although the goal of dataset distillation is to eliminate redundancies in the original dataset, the synthetic images themselves exhibit redundancy, causing saturation in high-IPC regions.

This lack of diversity significantly hampers the network's ability to generalize to unseen data, particularly in high-IPC settings, as illustrated in the graph. From these observations, we hypothesize that certain clusters of images are easily embedded into synthetic datasets, while other features of the original dataset are inherently difficult to learn. This issue has been noted in prior works, which have either exploited this property or attempted to mitigate it. Our approach focuses on the latter, while still leveraging the "easy learning" property during early training.

The core idea behind our methodology is that redundant features should not be generated, and hard-to-learn information from the original dataset must be embedded into the synthetic images. However, achieving this is challenging because synthetic images are typically generated in a single large batch, making it impossible to explicitly control which parts of the dataset are learned by individual images. To address this, we propose generating synthetic images in segments, enabling much greater control over the information embedded in each image.

But how do we decide which information to embed and which to exclude? Initially, we considered examining the feature space of generated images and grouping real images within a threshold distance of the synthetic images. While feasible, this approach would require numerous design decisions and extensive hyperparameter tuning. Instead, we devised a simpler and more intuitive solution: training a network with synthetic images and using the trained network as a discriminator on the real dataset to determine whether an image's information is embedded in the synthetic set. This approach allows us to train different batches of synthetic images with different subsets of the real dataset, ensuring that diverse and rich features of the original dataset are captured without redundancy.

Our method, implemented on IDC, demonstrates a substantial performance improvement, particularly in high-IPC regions, where it even surpasses the performance of networks trained on the full original dataset. Even more notably, our method shows great robustness across multiple unseen architectures, marking a significant step toward the broader practicality of dataset distillation.


\section{Related Works}
Dataset distillation compresses real images into synthetic images by optimizing the distance between the features of real and synthetic images. Current research primarily focuses on two strategies: gradient/trajectory matching and distribution matching.

\textbf{Gradient/Trajectory Matching}: The core principle of these methods is that synthetic images should produce back-propagation signals similar to those of real images. DC emphasizes matching one-step gradients, while MTT advances this concept by matching entire training trajectories. Although trajectory matching methods are often regarded as the current state-of-the-art (SOTA) framework for dataset distillation, our experiments reveal that gradient matching methods outperform them on large-scale datasets and high-IPC settings. Furthermore, gradient matching methods demonstrate superior generalization across multiple architectures, which is why our work is based on IDC.

\textbf{Distribution Matching}: Another approach focuses on aligning the feature maps of real and synthetic data. Early methods such as CAFE and DM adopted this strategy but exhibited inferior performance compared to gradient or trajectory matching frameworks. More recent works propose a novel take on distribution matching, generating synthetic images by aligning the batch normalization statistics of real images. However, these methods are highly memory-intensive, requiring approximately 40 times more memory to store soft labels than is needed for synthetic images. Moreover, synthetic images generated using this approach often perform worse than randomly selected images. Based on these findings, we conclude that this line of work is not well-suited for dataset distillation. Consequently, our focus remains on traditional gradient/trajectory matching methods.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\section{You \emph{can} have an appendix here.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
